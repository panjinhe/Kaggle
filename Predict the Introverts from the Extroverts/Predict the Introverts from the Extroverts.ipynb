{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-27T15:51:38.187658Z",
     "start_time": "2025-07-27T15:43:01.885016Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. 加载数据 ---\n",
    "# 加载训练集、测试集和外部原始数据集。\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "orig_df = pd.read_csv('personality_datasert.csv')\n",
    "\n",
    "# --- 2. 特征工程 ---\n",
    "# 定义用于合并的列。\n",
    "df_cols = [\n",
    "    'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance',\n",
    "    'Going_outside', 'Drained_after_socializing',\n",
    "    'Friends_circle_size', 'Post_frequency'\n",
    "]\n",
    "\n",
    "# 通过重命名列和删除重复项来准备原始数据集。\n",
    "# 这会创建一个参考数据帧，用于识别训练/测试集中的匹配记录。\n",
    "ref_df = (\n",
    "    orig_df.rename(columns={'Personality': 'match_p'})\n",
    "           .drop_duplicates(subset=df_cols)\n",
    ")\n",
    "\n",
    "# 用于将主数据帧与参考数据帧合并的函数。\n",
    "# 它会添加一个 'match_p' 列（原始的人格标签）和一个标志 'match_p_null'\n",
    "# 来指示是否在原始数据集中找到了匹配项。\n",
    "def merge_with_match_p(df, ref_df, merge_cols):\n",
    "    merged_df = df.merge(ref_df, how='left', on=merge_cols)\n",
    "    merged_df['match_p_null'] = merged_df['match_p'].isna().astype(int)\n",
    "    return merged_df\n",
    "\n",
    "# 将合并函数应用于训练集和测试集。\n",
    "train = merge_with_match_p(train_df, ref_df, df_cols)\n",
    "test = merge_with_match_p(test_df, ref_df, df_cols)\n",
    "\n",
    "# 删除合并过程中可能产生的任何重复行。\n",
    "train = train.drop_duplicates(subset=['id'], keep='first')\n",
    "test = test.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# 用 'unknown' 填充新的 'match_p' 列中的缺失值。\n",
    "train['match_p'] = train['match_p'].fillna('unknown')\n",
    "test['match_p'] = test['match_p'].fillna('unknown')\n",
    "\n",
    "# --- 3. 数据预处理 ---\n",
    "# 将目标变量 'Personality' 转换为二进制（1 代表外向，0 代表内向）。\n",
    "train['Personality'] = train['Personality'].map({'Extrovert': 1, 'Introvert': 0})\n",
    "\n",
    "# 定义要编码的分类列。\n",
    "cat_cols = ['match_p', 'Stage_fear', 'Drained_after_socializing']\n",
    "\n",
    "# 用于将分类字符串值映射为数值的函数。\n",
    "def cat_encode(df, columns):\n",
    "    for col in columns:\n",
    "        if col == 'match_p':\n",
    "            df[col] = df[col].map({'Extrovert': 2, 'Introvert': 1, 'unknown': 0})\n",
    "        else:\n",
    "            df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "    return df\n",
    "\n",
    "# 将编码应用于训练集和测试集。\n",
    "train = cat_encode(train, cat_cols)\n",
    "test = cat_encode(test, cat_cols)\n",
    "\n",
    "# 使用每列的平均值来填补数值列中的缺失值。\n",
    "for col in train.drop(columns=['id', 'Personality']).columns:\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "\n",
    "for col in test.drop(columns=['id']).columns:\n",
    "    test[col] = test[col].fillna(test[col].mean())\n",
    "\n",
    "# --- 4. 准备模型数据 ---\n",
    "# 分离特征（X）和目标（y）。\n",
    "X = train.drop(['Personality', 'id'], axis=1)\n",
    "y = train['Personality']\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# 计算 'scale_pos_weight' 以处理某些模型中的类别不平衡问题。\n",
    "# 这会在训练期间为少数类别提供更多权重。\n",
    "counter = Counter(y)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "\n",
    "# --- 5. 模型配置 ---\n",
    "# 为基础模型定义超参数。\n",
    "catboost_params = {\n",
    "    'iterations': 1000, 'learning_rate': 0.001, 'depth': 8, 'l2_leaf_reg': 3,\n",
    "    'border_count': 128, 'bagging_temperature': 1, 'random_strength': 1,\n",
    "    'loss_function': 'Logloss', 'eval_metric': 'AUC', 'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': 0, 'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary', 'n_estimators': 300, 'max_depth': 10, 'min_child_samples': 10,\n",
    "    'num_leaves': 20, 'learning_rate': 0.0109, 'colsample_bytree': 0.8819,\n",
    "    'subsample': 0.7015, 'scale_pos_weight': scale_pos_weight, 'metric': 'AUC',\n",
    "    'random_state': 42, 'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 642, 'learning_rate': 0.1337, 'max_depth': 8, 'subsample': 0.5003,\n",
    "    'colsample_bytree': 0.5300, 'gamma': 2.3324, 'reg_lambda': 2.4718, 'reg_alpha': 0.1671,\n",
    "    'min_child_weight': 8, 'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "    'use_label_encoder': False, 'scale_pos_weight': scale_pos_weight, 'random_state': 42\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 608, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 2,\n",
    "    'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced', 'random_state': 42\n",
    "}\n",
    "\n",
    "# --- 6. Stacking 集成模型 ---\n",
    "# 定义基础模型（第 0 层）。\n",
    "estimators = [\n",
    "    ('lgb', LGBMClassifier(**lgb_params)),\n",
    "    ('rf', RandomForestClassifier(**rf_params)),\n",
    "    ('xgb', XGBClassifier(**xgb_params)),\n",
    "    ('cat', CatBoostClassifier(**catboost_params))\n",
    "]\n",
    "\n",
    "# 定义元模型（第 1 层），它将从基础模型的预测中学习。\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 创建 Stacking 分类器。\n",
    "# 它会训练基础模型，然后在它们的输出上训练元模型。\n",
    "# passthrough=True 表示原始特征也会传递给元模型。\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    "    cv=5  # 用于训练基础模型的内部交叉验证\n",
    ")\n",
    "\n",
    "# --- 7. 交叉验证和预测 ---\n",
    "# 设置分层 K 折交叉验证，以在每个折中保持目标分布。\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "# 循环遍历每个折以进行训练和验证。\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # 在训练折上训练 Stacking 分类器。\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "    # 在验证集上进行预测以评估性能。\n",
    "    y_pred_proba = stacking_clf.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    # 如果需要，您可以在此处添加其他指标，如准确率。\n",
    "\n",
    "    # 累积对测试集的预测。每个折都会对最终预测做出贡献。\n",
    "    test_predictions += stacking_clf.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# --- 8. 生成提交文件 ---\n",
    "# 使用 0.5 的阈值将最终的平均概率转换为类别标签（0 或 1）。\n",
    "final_predictions_int = (test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# 将数值标签映射回原始的字符串标签。\n",
    "label_map = {1: 'Extrovert', 0: 'Introvert'}\n",
    "final_predictions_labeled = pd.Series(final_predictions_int).map(label_map)\n",
    "\n",
    "# 以所需格式创建提交的 DataFrame。\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'Personality': final_predictions_labeled})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 156\u001B[0m\n\u001B[0;32m    153\u001B[0m y_train, y_val \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39miloc[train_index], y\u001B[38;5;241m.\u001B[39miloc[val_index]\n\u001B[0;32m    155\u001B[0m \u001B[38;5;66;03m# 在训练折上训练 Stacking 分类器。\u001B[39;00m\n\u001B[1;32m--> 156\u001B[0m stacking_clf\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m    158\u001B[0m \u001B[38;5;66;03m# 在验证集上进行预测以评估性能。\u001B[39;00m\n\u001B[0;32m    159\u001B[0m y_pred_proba \u001B[38;5;241m=\u001B[39m stacking_clf\u001B[38;5;241m.\u001B[39mpredict_proba(X_val)[:, \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001B[0m, in \u001B[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m extra_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(all_args)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extra_args \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# extra_args > 0\u001B[39;00m\n\u001B[0;32m     66\u001B[0m args_msg \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name, arg)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(kwonly_args[:extra_args], args[\u001B[38;5;241m-\u001B[39mextra_args:])\n\u001B[0;32m     69\u001B[0m ]\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:717\u001B[0m, in \u001B[0;36mStackingClassifier.fit\u001B[1;34m(self, X, y, sample_weight, **fit_params)\u001B[0m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    716\u001B[0m     fit_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m sample_weight\n\u001B[1;32m--> 717\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(X, y_encoded, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:254\u001B[0m, in \u001B[0;36m_BaseStacking.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(cv, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandom_state\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m cv\u001B[38;5;241m.\u001B[39mrandom_state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    252\u001B[0m         cv\u001B[38;5;241m.\u001B[39mrandom_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mRandomState()\n\u001B[1;32m--> 254\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)(\n\u001B[0;32m    255\u001B[0m         delayed(cross_val_predict)(\n\u001B[0;32m    256\u001B[0m             clone(est),\n\u001B[0;32m    257\u001B[0m             X,\n\u001B[0;32m    258\u001B[0m             y,\n\u001B[0;32m    259\u001B[0m             cv\u001B[38;5;241m=\u001B[39mdeepcopy(cv),\n\u001B[0;32m    260\u001B[0m             method\u001B[38;5;241m=\u001B[39mmeth,\n\u001B[0;32m    261\u001B[0m             n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    262\u001B[0m             params\u001B[38;5;241m=\u001B[39mrouted_params[name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    263\u001B[0m             verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    264\u001B[0m         )\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m name, est, meth \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, all_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_method_)\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m est \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m     )\n\u001B[0;32m    269\u001B[0m \u001B[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001B[39;00m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;66;03m# Remove the None from the method as well.\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_method_ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    272\u001B[0m     meth\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (meth, est) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_method_, all_estimators)\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m est \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    275\u001B[0m ]\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1247\u001B[0m, in \u001B[0;36mcross_val_predict\u001B[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001B[0m\n\u001B[0;32m   1244\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m   1246\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m-> 1247\u001B[0m predictions \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m   1248\u001B[0m     delayed(_fit_and_predict)(\n\u001B[0;32m   1249\u001B[0m         clone(estimator),\n\u001B[0;32m   1250\u001B[0m         X,\n\u001B[0;32m   1251\u001B[0m         y,\n\u001B[0;32m   1252\u001B[0m         train,\n\u001B[0;32m   1253\u001B[0m         test,\n\u001B[0;32m   1254\u001B[0m         routed_params\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mfit,\n\u001B[0;32m   1255\u001B[0m         method,\n\u001B[0;32m   1256\u001B[0m     )\n\u001B[0;32m   1257\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m splits\n\u001B[0;32m   1258\u001B[0m )\n\u001B[0;32m   1260\u001B[0m inv_test_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;28mlen\u001B[39m(test_indices), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m   1261\u001B[0m inv_test_indices[test_indices] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(test_indices))\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1332\u001B[0m, in \u001B[0;36m_fit_and_predict\u001B[1;34m(estimator, X, y, train, test, fit_params, method)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1332\u001B[0m     estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m   1333\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(estimator, method)\n\u001B[0;32m   1334\u001B[0m predictions \u001B[38;5;241m=\u001B[39m func(X_test)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    476\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    477\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    479\u001B[0m ]\n\u001B[0;32m    481\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 487\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    488\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    489\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    490\u001B[0m     prefer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthreads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    491\u001B[0m )(\n\u001B[0;32m    492\u001B[0m     delayed(_parallel_build_trees)(\n\u001B[0;32m    493\u001B[0m         t,\n\u001B[0;32m    494\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbootstrap,\n\u001B[0;32m    495\u001B[0m         X,\n\u001B[0;32m    496\u001B[0m         y,\n\u001B[0;32m    497\u001B[0m         sample_weight,\n\u001B[0;32m    498\u001B[0m         i,\n\u001B[0;32m    499\u001B[0m         \u001B[38;5;28mlen\u001B[39m(trees),\n\u001B[0;32m    500\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    501\u001B[0m         class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight,\n\u001B[0;32m    502\u001B[0m         n_samples_bootstrap\u001B[38;5;241m=\u001B[39mn_samples_bootstrap,\n\u001B[0;32m    503\u001B[0m         missing_values_in_feature_mask\u001B[38;5;241m=\u001B[39mmissing_values_in_feature_mask,\n\u001B[0;32m    504\u001B[0m     )\n\u001B[0;32m    505\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    506\u001B[0m )\n\u001B[0;32m    508\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:176\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    174\u001B[0m     curr_sample_weight \u001B[38;5;241m=\u001B[39m sample_weight\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m--> 176\u001B[0m indices \u001B[38;5;241m=\u001B[39m _generate_sample_indices(\n\u001B[0;32m    177\u001B[0m     tree\u001B[38;5;241m.\u001B[39mrandom_state, n_samples, n_samples_bootstrap\n\u001B[0;32m    178\u001B[0m )\n\u001B[0;32m    179\u001B[0m sample_counts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(indices, minlength\u001B[38;5;241m=\u001B[39mn_samples)\n\u001B[0;32m    180\u001B[0m curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m sample_counts\n",
      "File \u001B[1;32mD:\\Program Files\\Anaconda\\envs\\py312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:130\u001B[0m, in \u001B[0;36m_generate_sample_indices\u001B[1;34m(random_state, n_samples, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03mPrivate function used to _parallel_build_trees function.\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m random_instance \u001B[38;5;241m=\u001B[39m check_random_state(random_state)\n\u001B[1;32m--> 130\u001B[0m sample_indices \u001B[38;5;241m=\u001B[39m random_instance\u001B[38;5;241m.\u001B[39mrandint(\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;241m0\u001B[39m, n_samples, n_samples_bootstrap, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32\n\u001B[0;32m    132\u001B[0m )\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sample_indices\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T16:11:12.125746Z",
     "start_time": "2025-07-27T15:51:44.388999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. 加载数据 ---\n",
    "# 加载训练集、测试集和外部原始数据集。\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "orig_df = pd.read_csv('personality_datasert.csv')\n",
    "\n",
    "# --- 2. 特征工程 ---\n",
    "# 定义用于合并的列。\n",
    "df_cols = [\n",
    "    'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance',\n",
    "    'Going_outside', 'Drained_after_socializing',\n",
    "    'Friends_circle_size', 'Post_frequency'\n",
    "]\n",
    "\n",
    "# 通过重命名列和删除重复项来准备原始数据集。\n",
    "# 这会创建一个参考数据帧，用于识别训练/测试集中的匹配记录。\n",
    "ref_df = (\n",
    "    orig_df.rename(columns={'Personality': 'match_p'})\n",
    "           .drop_duplicates(subset=df_cols)\n",
    ")\n",
    "\n",
    "# 用于将主数据帧与参考数据帧合并的函数。\n",
    "# 它会添加一个 'match_p' 列（原始的人格标签）和一个标志 'match_p_null'\n",
    "# 来指示是否在原始数据集中找到了匹配项。\n",
    "def merge_with_match_p(df, ref_df, merge_cols):\n",
    "    merged_df = df.merge(ref_df, how='left', on=merge_cols)\n",
    "    merged_df['match_p_null'] = merged_df['match_p'].isna().astype(int)\n",
    "    return merged_df\n",
    "\n",
    "# 将合并函数应用于训练集和测试集。\n",
    "train = merge_with_match_p(train_df, ref_df, df_cols)\n",
    "test = merge_with_match_p(test_df, ref_df, df_cols)\n",
    "\n",
    "# 删除合并过程中可能产生的任何重复行。\n",
    "train = train.drop_duplicates(subset=['id'], keep='first')\n",
    "test = test.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# 用 'unknown' 填充新的 'match_p' 列中的缺失值。\n",
    "train['match_p'] = train['match_p'].fillna('unknown')\n",
    "test['match_p'] = test['match_p'].fillna('unknown')\n",
    "\n",
    "# --- 3. 数据预处理 ---\n",
    "# 将目标变量 'Personality' 转换为二进制（1 代表外向，0 代表内向）。\n",
    "train['Personality'] = train['Personality'].map({'Extrovert': 1, 'Introvert': 0})\n",
    "\n",
    "# 定义要编码的分类列。\n",
    "cat_cols = ['match_p', 'Stage_fear', 'Drained_after_socializing']\n",
    "\n",
    "# 用于将分类字符串值映射为数值的函数。\n",
    "def cat_encode(df, columns):\n",
    "    for col in columns:\n",
    "        if col == 'match_p':\n",
    "            df[col] = df[col].map({'Extrovert': 2, 'Introvert': 1, 'unknown': 0})\n",
    "        else:\n",
    "            df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "    return df\n",
    "\n",
    "# 将编码应用于训练集和测试集。\n",
    "train = cat_encode(train, cat_cols)\n",
    "test = cat_encode(test, cat_cols)\n",
    "\n",
    "# 使用每列的平均值来填补数值列中的缺失值。\n",
    "for col in train.drop(columns=['id', 'Personality']).columns:\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "\n",
    "for col in test.drop(columns=['id']).columns:\n",
    "    test[col] = test[col].fillna(test[col].mean())\n",
    "\n",
    "# --- 4. 准备模型数据 ---\n",
    "# 分离特征（X）和目标（y）。\n",
    "X = train.drop(['Personality', 'id'], axis=1)\n",
    "y = train['Personality']\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# 计算 'scale_pos_weight' 以处理某些模型中的类别不平衡问题。\n",
    "# 这会在训练期间为少数类别提供更多权重。\n",
    "counter = Counter(y)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "\n",
    "# --- 5. 模型配置 ---\n",
    "# 为基础模型定义超参数。\n",
    "catboost_params = {\n",
    "    'iterations': 1000, 'learning_rate': 0.001, 'depth': 8, 'l2_leaf_reg': 3,\n",
    "    'border_count': 128, 'bagging_temperature': 1, 'random_strength': 1,\n",
    "    'loss_function': 'Logloss', 'eval_metric': 'AUC', 'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': 0, 'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary', 'n_estimators': 300, 'max_depth': 10, 'min_child_samples': 10,\n",
    "    'num_leaves': 20, 'learning_rate': 0.0109, 'colsample_bytree': 0.8819,\n",
    "    'subsample': 0.7015, 'scale_pos_weight': scale_pos_weight, 'metric': 'AUC',\n",
    "    'random_state': 42, 'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 642, 'learning_rate': 0.1337, 'max_depth': 8, 'subsample': 0.5003,\n",
    "    'colsample_bytree': 0.5300, 'gamma': 2.3324, 'reg_lambda': 2.4718, 'reg_alpha': 0.1671,\n",
    "    'min_child_weight': 8, 'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "    'use_label_encoder': False, 'scale_pos_weight': scale_pos_weight, 'random_state': 42\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 608, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 2,\n",
    "    'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced', 'random_state': 42\n",
    "}\n",
    "\n",
    "# --- 6. Stacking 集成模型 ---\n",
    "# 定义基础模型（第 0 层）。\n",
    "estimators = [\n",
    "    ('lgb', LGBMClassifier(**lgb_params)),\n",
    "    ('rf', RandomForestClassifier(**rf_params)),\n",
    "    ('xgb', XGBClassifier(**xgb_params)),\n",
    "    ('cat', CatBoostClassifier(**catboost_params))\n",
    "]\n",
    "\n",
    "# 定义元模型（第 1 层），它将从基础模型的预测中学习。\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 创建 Stacking 分类器。\n",
    "# 它会训练基础模型，然后在它们的输出上训练元模型。\n",
    "# passthrough=True 表示原始特征也会传递给元模型。\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    "    cv=5  # 用于训练基础模型的内部交叉验证\n",
    ")\n",
    "\n",
    "# --- 7. 交叉验证和预测 ---\n",
    "# 设置分层 K 折交叉验证，以在每个折中保持目标分布。\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "# 循环遍历每个折以进行训练和验证。\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # 在训练折上训练 Stacking 分类器。\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "    # 在验证集上进行预测以评估性能。\n",
    "    y_pred_proba = stacking_clf.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    # 如果需要，您可以在此处添加其他指标，如准确率。\n",
    "\n",
    "    # 累积对测试集的预测。每个折都会对最终预测做出贡献。\n",
    "    test_predictions += stacking_clf.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# --- 8. 生成提交文件 ---\n",
    "# 使用 0.5 的阈值将最终的平均概率转换为类别标签（0 或 1）。\n",
    "final_predictions_int = (test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# 将数值标签映射回原始的字符串标签。\n",
    "label_map = {1: 'Extrovert', 0: 'Introvert'}\n",
    "final_predictions_labeled = pd.Series(final_predictions_int).map(label_map)\n",
    "\n",
    "# 以所需格式创建提交的 DataFrame。\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'Personality': final_predictions_labeled})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ],
   "id": "87f01cfb1607cb6b",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312] *",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
