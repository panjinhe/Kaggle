{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:41:00.623810Z",
     "start_time": "2025-07-27T10:40:56.423050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. 加载所有数据文件 ---\n",
    "print(\"⚙️ 步骤 1: 正在加载数据...\")\n",
    "base_path = 'E:\\Kaggle\\Store Sales - Time Series Forecasting/'\n",
    "train_df = pd.read_csv(f'{base_path}train.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv(f'{base_path}test.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv(f'{base_path}stores.csv')\n",
    "oil_df = pd.read_csv(f'{base_path}oil.csv', parse_dates=['date'])\n",
    "holidays_df = pd.read_csv(f'{base_path}holidays_events.csv', parse_dates=['date'])\n",
    "transactions_df = pd.read_csv(f'{base_path}transactions.csv', parse_dates=['date'])\n",
    "print(\"✅ 数据加载成功！\")\n",
    "\n",
    "# --- 2. 合并数据 ---\n",
    "print(\"\\n⚙️ 步骤 2: 正在合并数据...\")\n",
    "# 合并训练集\n",
    "df_train_full = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "df_train_full = df_train_full.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "df_train_full = df_train_full.merge(oil_df, on='date', how='left')\n",
    "\n",
    "# 合并测试集\n",
    "df_test_full = test_df.merge(stores_df, on='store_nbr', how='left')\n",
    "df_test_full = df_test_full.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "df_test_full = df_test_full.merge(oil_df, on='date', how='left')\n",
    "print(\"✅ 数据合并成功！\")\n",
    "\n",
    "# --- 3. 填充缺失值 ---\n",
    "print(\"\\n⚙️ 步骤 3: 正在填充缺失值...\")\n",
    "for df in [df_train_full, df_test_full]:\n",
    "    df['dcoilwtico'] = df['dcoilwtico'].ffill().bfill()\n",
    "    df['transactions'] = df['transactions'].fillna(0)\n",
    "print(\"✅ 缺失值填充成功！\")"
   ],
   "id": "e3603ca71cff4be6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ 步骤 1: 正在加载数据...\n",
      "✅ 数据加载成功！\n",
      "\n",
      "⚙️ 步骤 2: 正在合并数据...\n",
      "✅ 数据合并成功！\n",
      "\n",
      "⚙️ 步骤 3: 正在填充缺失值...\n",
      "✅ 缺失值填充成功！\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:41:22.766050Z",
     "start_time": "2025-07-27T10:41:05.166139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 4. Advanced Feature Engineering (Inspired by R script) ---\n",
    "print(\"\\n⚙️ 步骤 4: 正在进行高级特征工程...\")\n",
    "\n",
    "# 合并训练和测试集，以便统一处理。'sales' 列被保留用于创建滞后和编码特征。\n",
    "df_combined = pd.concat([df_train_full, df_test_full], sort=False, ignore_index=True)\n",
    "df_combined = df_combined.sort_values(by=['store_nbr', 'family', 'date'])\n",
    "\n",
    "\n",
    "# --- Part A: Date-based & Holiday Features ---\n",
    "print(\"    - 创建日期、趋势、傅里叶和节假日特征...\")\n",
    "# 创建一个包含所有日期的骨架，以计算不依赖于分组的特征\n",
    "all_dates = pd.DataFrame({'date': df_combined['date'].unique()}).sort_values('date')\n",
    "\n",
    "# 1. 趋势特征\n",
    "all_dates['trend'] = np.arange(len(all_dates))\n",
    "\n",
    "# 2. 傅里叶特征 (Fourier Features) for Seasonality\n",
    "K = 2 # 使用2个谐波\n",
    "for k in range(1, K + 1):\n",
    "    # Yearly seasonality\n",
    "    all_dates[f'fourier_sin_y{k}'] = np.sin(2 * np.pi * k * all_dates['date'].dt.dayofyear / 365.25)\n",
    "    all_dates[f'fourier_cos_y{k}'] = np.cos(2 * np.pi * k * all_dates['date'].dt.dayofyear / 365.25)\n",
    "    # Weekly seasonality\n",
    "    all_dates[f'fourier_sin_w{k}'] = np.sin(2 * np.pi * k * all_dates['date'].dt.dayofweek / 7)\n",
    "    all_dates[f'fourier_cos_w{k}'] = np.cos(2 * np.pi * k * all_dates['date'].dt.dayofweek / 7)\n",
    "\n",
    "# 3. 高级节假日特征\n",
    "h_df = holidays_df[holidays_df['transferred'] == False].copy()\n",
    "h_df_national = h_df[h_df['locale'] == 'National'].rename(columns={'description': 'holiday_national'})[['date', 'holiday_national']]\n",
    "h_df_regional = h_df[h_df['locale'] == 'Regional'].rename(columns={'description': 'holiday_regional'})[['date', 'holiday_regional']]\n",
    "h_df_local = h_df[h_df['locale'] == 'Local'].rename(columns={'description': 'holiday_local'})[['date', 'holiday_local']]\n",
    "\n",
    "# 创建标志位\n",
    "h_df_national['is_national_holiday'] = 1\n",
    "h_df_regional['is_regional_holiday'] = 1\n",
    "h_df_local['is_local_holiday'] = 1\n",
    "\n",
    "# 合并节假日标志\n",
    "all_dates = all_dates.merge(h_df_national.groupby('date').first(), on='date', how='left')\n",
    "all_dates = all_dates.merge(h_df_regional.groupby('date').first(), on='date', how='left')\n",
    "all_dates = all_dates.merge(h_df_local.groupby('date').first(), on='date', how='left')\n",
    "holiday_cols = ['is_national_holiday', 'is_regional_holiday', 'is_local_holiday']\n",
    "all_dates[holiday_cols] = all_dates[holiday_cols].fillna(0)\n",
    "\n",
    "# 4. 合并所有基于日期的特征到主数据框\n",
    "df_featured = df_combined.merge(all_dates.drop(['holiday_national', 'holiday_regional', 'holiday_local'], axis=1, errors='ignore'), on='date', how='left')\n",
    "\n",
    "# 5. 其他常规日期特征\n",
    "df_featured['dayofweek'] = df_featured['date'].dt.dayofweek\n",
    "df_featured['month'] = df_featured['date'].dt.month\n",
    "df_featured['is_payday'] = ((df_featured['date'].dt.day == 15) | (df_featured['date'].dt.is_month_end)).astype(int)\n",
    "\n",
    "\n",
    "# --- Part B: Target Encoding ---\n",
    "print(\"    - 创建目标编码特征...\")\n",
    "# 使用2017-07-01之前的数据进行编码，防止数据泄露\n",
    "encoding_cutoff = \"2017-07-01\"\n",
    "temp_df = df_featured[df_featured['date'] < encoding_cutoff]\n",
    "\n",
    "# 计算均值\n",
    "family_mean = temp_df.groupby('family')['sales'].mean().to_dict()\n",
    "store_family_mean = temp_df.groupby(['store_nbr', 'family'])['sales'].mean().to_dict()\n",
    "\n",
    "# 映射到特征\n",
    "df_featured['enc_family_mean'] = df_featured['family'].map(family_mean)\n",
    "df_featured['enc_store_family_mean'] = df_featured.set_index(['store_nbr', 'family']).index.map(store_family_mean)\n",
    "\n",
    "# 填充可能因新组合产生的NaN\n",
    "df_featured['enc_family_mean'].fillna(np.mean(list(family_mean.values())), inplace=True)\n",
    "df_featured['enc_store_family_mean'].fillna(df_featured['enc_family_mean'], inplace=True)\n",
    "\n",
    "\n",
    "# --- Part C: Lag & Rolling Features ---\n",
    "print(\"    - 创建滞后和滚动特征...\")\n",
    "# 重新排序以确保分组计算正确\n",
    "df_featured = df_featured.sort_values(by=['store_nbr', 'family', 'date'])\n",
    "\n",
    "# 定义要创建特征的列和参数\n",
    "lag_features = {\n",
    "    'sales': [7, 14, 28],\n",
    "    'onpromotion': [1, 7, 14],\n",
    "}\n",
    "rolling_features = {\n",
    "    'sales': [7, 14, 28],\n",
    "    'onpromotion': [7, 14],\n",
    "    'transactions': [7, 14]\n",
    "}\n",
    "\n",
    "# 创建滞后特征\n",
    "for col, lags in lag_features.items():\n",
    "    for lag in lags:\n",
    "        df_featured[f'{col}_lag_{lag}'] = df_featured.groupby(['store_nbr', 'family'])[col].shift(lag)\n",
    "\n",
    "# 创建滚动特征\n",
    "for col, windows in rolling_features.items():\n",
    "    for window in windows:\n",
    "        # 使用shift(1)来防止在计算中使用当天的数据\n",
    "        shifted_data = df_featured.groupby(['store_nbr', 'family'])[col].shift(1)\n",
    "        df_featured[f'{col}_rolling_mean_{window}'] = shifted_data.rolling(window).mean()\n",
    "        df_featured[f'{col}_rolling_std_{window}'] = shifted_data.rolling(window).std()\n",
    "\n",
    "# --- Part D: Final Imputation & Split ---\n",
    "print(\"    - 进行最终填充和数据分割...\")\n",
    "# 填充因滞后和滚动产生的NaN值\n",
    "# 首先用前向填充处理组内的NaN，然后用0填充剩余的（通常是每个组的开头）\n",
    "df_featured.fillna(method='ffill', inplace=True)\n",
    "df_featured.fillna(0, inplace=True)\n",
    "\n",
    "# 分离回最终的训练集和测试集\n",
    "df_train_final = df_featured[df_featured['id'].isin(train_df['id'])].copy()\n",
    "df_test_final = df_featured[df_featured['id'].isin(test_df['id'])].copy()\n",
    "\n",
    "print(\"✅ 高级特征工程完成！\")\n",
    "print(\"最终训练集形状:\", df_train_final.shape)\n",
    "print(\"最终测试集形状:\", df_test_final.shape)"
   ],
   "id": "e16a458d612ef79d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ 步骤 4: 正在进行高级特征工程...\n",
      "    - 创建日期、趋势、傅里叶和节假日特征...\n",
      "    - 创建目标编码特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_38552\\2464560915.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_featured['enc_family_mean'].fillna(np.mean(list(family_mean.values())), inplace=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_38552\\2464560915.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_featured['enc_store_family_mean'].fillna(df_featured['enc_family_mean'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - 创建滞后和滚动特征...\n",
      "    - 进行最终填充和数据分割...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_38552\\2464560915.py:109: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_featured.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 高级特征工程完成！\n",
      "最终训练集形状: (3000888, 49)\n",
      "最终测试集形状: (28512, 49)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T12:26:33.717081Z",
     "start_time": "2025-07-27T11:57:10.352613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# 确保 Optuna 的日志级别设置为 WARNING，避免过多输出\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- 1. 准备最终的特征和目标 ---\n",
    "print(\"⚙️ 从最终数据中分离目标变量和特征...\")\n",
    "features = [col for col in df_train_final.columns if col not in ['id', 'date', 'sales']]\n",
    "test_ids = df_test_final['id']\n",
    "X_train = df_train_final[features].copy()\n",
    "X_test = df_test_final[features].copy()\n",
    "\n",
    "# --- 2. 为 LightGBM 准备数据 ---\n",
    "print(\"⚙️ 准备 LightGBM 数据格式...\")\n",
    "categorical_features = [\n",
    "    \"store_nbr\", \"family\", \"city\", \"state\", \"type\", \"dayofweek\", \"month\",\n",
    "    \"is_national_holiday\", \"is_regional_holiday\", \"is_local_holiday\"\n",
    "]\n",
    "categorical_features = [f for f in categorical_features if f in features]\n",
    "\n",
    "print(\"    - 转换分类特征的数据类型为 'category'...\")\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# --- 3. 创建基于时间的验证集 ---\n",
    "print(\"⚙️ 创建验证集...\")\n",
    "val_start_date = '2017-07-26'\n",
    "val_mask = df_train_final['date'] >= val_start_date\n",
    "\n",
    "# ===================================================================\n",
    "#  模型 1: 分类器 (使用 Optuna 进行贝叶斯优化)\n",
    "# ===================================================================\n",
    "print(\"\\n--- 开始优化模型 1: 分类器 ---\")\n",
    "y_classifier = (df_train_final['sales'] > 0).astype(int)\n",
    "X_train_clf, X_val_clf = X_train[~val_mask], X_train[val_mask]\n",
    "y_train_clf, y_val_clf = y_classifier[~val_mask], y_classifier[val_mask]\n",
    "\n",
    "# *** 加速优化 1: 为 Optuna 创建一个数据子样本 ***\n",
    "# 在耗时的调优阶段使用部分数据，可以极大提升速度\n",
    "print(\"    - 为加速优化，创建数据子样本...\")\n",
    "N_OPTUNA_SAMPLES = 750000\n",
    "optuna_clf_indices = np.random.choice(X_train_clf.index, N_OPTUNA_SAMPLES, replace=False)\n",
    "X_train_clf_opt = X_train_clf.loc[optuna_clf_indices]\n",
    "y_train_clf_opt = y_train_clf.loc[optuna_clf_indices]\n",
    "\n",
    "\n",
    "def objective_classifier(trial):\n",
    "    \"\"\"Optuna 的目标函数，用于寻找分类器的最佳参数\"\"\"\n",
    "    # LightGBM 默认使用高效的直方图算法\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 80),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params, n_estimators=1000)\n",
    "    # 使用子样本进行训练\n",
    "    model.fit(X_train_clf_opt, y_train_clf_opt,\n",
    "              eval_set=[(X_val_clf, y_val_clf)],\n",
    "              eval_metric='logloss',\n",
    "              callbacks=[lgb.early_stopping(15, verbose=False)])\n",
    "\n",
    "    return model.best_score_['valid_0']['binary_logloss']\n",
    "\n",
    "# *** 加速优化 2: 减少试验次数 ***\n",
    "N_TRIALS = 10\n",
    "study_clf = optuna.create_study(direction='minimize')\n",
    "study_clf.optimize(objective_classifier, n_trials=N_TRIALS)\n",
    "best_params_clf = study_clf.best_params\n",
    "print(f\"✅ 分类器优化完成！最佳 Logloss: {study_clf.best_value:.5f}\")\n",
    "\n",
    "# ===================================================================\n",
    "#  模型 2: 回归器 (使用 Optuna 进行贝叶斯优化)\n",
    "# ===================================================================\n",
    "print(\"\\n--- 开始优化模型 2: 回归器 ---\")\n",
    "train_positive_mask = df_train_final['sales'] > 0\n",
    "df_train_positive = df_train_final[train_positive_mask]\n",
    "val_mask_positive = df_train_positive['date'] >= val_start_date\n",
    "X_train_reg_full = df_train_positive[~val_mask_positive][features]\n",
    "y_train_reg_full = np.log1p(df_train_positive[~val_mask_positive]['sales'])\n",
    "X_val_reg = df_train_positive[val_mask_positive][features]\n",
    "y_val_reg = np.log1p(df_train_positive[val_mask_positive]['sales'])\n",
    "\n",
    "# 为回归器创建优化子样本\n",
    "optuna_reg_indices = np.random.choice(X_train_reg_full.index, min(N_OPTUNA_SAMPLES, len(X_train_reg_full)), replace=False)\n",
    "X_train_reg_opt = X_train_reg_full.loc[optuna_reg_indices]\n",
    "y_train_reg_opt = y_train_reg_full.loc[optuna_reg_indices]\n",
    "\n",
    "for col in categorical_features:\n",
    "    X_train_reg_opt[col] = X_train_reg_opt[col].astype('category')\n",
    "    X_val_reg[col] = X_val_reg[col].astype('category')\n",
    "\n",
    "\n",
    "def objective_regressor(trial):\n",
    "    \"\"\"Optuna 的目标函数，用于寻找回归器的最佳参数\"\"\"\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators=2000)\n",
    "    model.fit(X_train_reg_opt, y_train_reg_opt,\n",
    "              eval_set=[(X_val_reg, y_val_reg)],\n",
    "              eval_metric='rmse',\n",
    "              callbacks=[lgb.early_stopping(15, verbose=False)])\n",
    "\n",
    "    return model.best_score_['valid_0']['rmse']\n",
    "\n",
    "study_reg = optuna.create_study(direction='minimize')\n",
    "study_reg.optimize(objective_regressor, n_trials=N_TRIALS)\n",
    "best_params_reg = study_reg.best_params\n",
    "print(f\"✅ 回归器优化完成！最佳 RMSE: {study_reg.best_value:.5f}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  使用找到的最佳参数在【完整数据】上训练最终模型\n",
    "# ===================================================================\n",
    "print(\"\\n--- 使用最佳参数在【完整数据】上训练最终模型 ---\")\n",
    "\n",
    "final_clf_params = {**best_params_clf, 'objective': 'binary', 'metric': 'logloss', 'seed': 42, 'verbose': -1, 'n_jobs': -1}\n",
    "final_reg_params = {**best_params_reg, 'objective': 'regression_l1', 'metric': 'rmse', 'seed': 42, 'verbose': -1, 'n_jobs': -1}\n",
    "\n",
    "# 训练最终分类器 (使用完整训练集)\n",
    "final_classifier = lgb.LGBMClassifier(**final_clf_params, n_estimators=1000)\n",
    "final_classifier.fit(X_train_clf, y_train_clf,\n",
    "                     eval_set=[(X_val_clf, y_val_clf)],\n",
    "                     eval_metric='logloss',\n",
    "                     callbacks=[lgb.early_stopping(15, verbose=False)])\n",
    "\n",
    "# 训练最终回归器 (使用完整正销量训练集)\n",
    "for col in categorical_features:\n",
    "    X_train_reg_full[col] = X_train_reg_full[col].astype('category')\n",
    "\n",
    "final_regressor = lgb.LGBMRegressor(**final_reg_params, n_estimators=2000)\n",
    "final_regressor.fit(X_train_reg_full, y_train_reg_full,\n",
    "                    eval_set=[(X_val_reg, y_val_reg)],\n",
    "                    eval_metric='rmse',\n",
    "                    callbacks=[lgb.early_stopping(15, verbose=False)])\n",
    "\n",
    "# ===================================================================\n",
    "#  最终预测: 合并两个模型的结果 (已修正)\n",
    "# ===================================================================\n",
    "print(\"\\n⚙️ 开始生成并合并预测...\")\n",
    "\n",
    "# *** 已修正：采用更稳健的预测逻辑 ***\n",
    "\n",
    "# 1. 直接用回归器预测对数转换后的销售额\n",
    "#    这个预测结果本身就是为了优化 RMSLE\n",
    "log_amount_pred = final_regressor.predict(X_test)\n",
    "\n",
    "# 2. 将预测值转换回原始尺度\n",
    "final_predictions = np.expm1(log_amount_pred)\n",
    "\n",
    "# 3. 将所有负数预测修正为 0\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "# (可选但推荐) 使用分类器进行修正：\n",
    "# 对于分类器预测概率极低的项，直接将其销量置为0，这可以修正回归器的一些极端预测\n",
    "# prob_of_sale = final_classifier.predict_proba(X_test)[:, 1]\n",
    "# final_predictions[prob_of_sale < 0.01] = 0 # 阈值可以调整\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'sales': final_predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ 提交文件 'submission.csv' 已成功生成！\")\n",
    "print(\"提交文件预览:\")\n",
    "print(submission_df.head())\n"
   ],
   "id": "ef68c5d31d6f16de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ 从最终数据中分离目标变量和特征...\n",
      "⚙️ 准备 LightGBM 数据格式...\n",
      "    - 转换分类特征的数据类型为 'category'...\n",
      "⚙️ 创建验证集...\n",
      "\n",
      "--- 开始优化模型 1: 分类器 ---\n",
      "    - 为加速优化，创建数据子样本...\n",
      "✅ 分类器优化完成！最佳 Logloss: 0.09656\n",
      "\n",
      "--- 开始优化模型 2: 回归器 ---\n",
      "✅ 回归器优化完成！最佳 RMSE: 0.35250\n",
      "\n",
      "--- 使用最佳参数在【完整数据】上训练最终模型 ---\n",
      "\n",
      "⚙️ 开始生成并合并预测...\n",
      "\n",
      "✅ 提交文件 'submission.csv' 已成功生成！\n",
      "提交文件预览:\n",
      "           id     sales\n",
      "1684  3000888  2.875436\n",
      "1685  3002670  2.911248\n",
      "1686  3004452  3.030787\n",
      "1687  3006234  3.793828\n",
      "1688  3008016  3.230670\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:43:28.938722Z",
     "start_time": "2025-07-27T12:37:12.322102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 1: 增强特征工程\n",
    "# ===================================================================\n",
    "print(\"\\n⚙️ 步骤 1: 正在进行增强特征工程...\")\n",
    "\n",
    "# 合并训练和测试集，以便统一处理。'sales' 列被保留用于创建滞后和编码特征。\n",
    "df_combined = pd.concat([df_train_full, df_test_full], sort=False, ignore_index=True)\n",
    "df_combined = df_combined.sort_values(by=['store_nbr', 'family', 'date'])\n",
    "\n",
    "\n",
    "# --- Part A: Date, Trend, and Fourier Features ---\n",
    "print(\"    - 创建日期、趋势和傅里叶特征...\")\n",
    "all_dates = pd.DataFrame({'date': df_combined['date'].unique()}).sort_values('date')\n",
    "all_dates['trend'] = np.arange(len(all_dates))\n",
    "K = 2\n",
    "for k in range(1, K + 1):\n",
    "    all_dates[f'fourier_sin_y{k}'] = np.sin(2 * np.pi * k * all_dates['date'].dt.dayofyear / 365.25)\n",
    "    all_dates[f'fourier_cos_y{k}'] = np.cos(2 * np.pi * k * all_dates['date'].dt.dayofyear / 365.25)\n",
    "df_featured = df_combined.merge(all_dates, on='date', how='left')\n",
    "df_featured['dayofweek'] = df_featured['date'].dt.dayofweek\n",
    "df_featured['month'] = df_featured['date'].dt.month\n",
    "df_featured['is_payday'] = ((df_featured['date'].dt.day == 15) | (df_featured['date'].dt.is_month_end)).astype(int)\n",
    "\n",
    "\n",
    "# --- Part B: Holiday Feature Engineering (新功能：基于相关性的筛选和独热编码) ---\n",
    "print(\"    - 创建基于相关性的假日特征...\")\n",
    "# 1. 准备用于计算相关性的数据\n",
    "holidays_filtered = holidays_df[holidays_df['transferred'] == False].copy()\n",
    "hol_sales = df_featured.merge(holidays_filtered[['date', 'description']], on='date', how='left')\n",
    "hol_sales = hol_sales[hol_sales['description'].notna()]\n",
    "\n",
    "# 2. 计算每个假日与销量的相关性\n",
    "correlations = {}\n",
    "for holiday in hol_sales['description'].unique():\n",
    "    holiday_indicator = (hol_sales['description'] == holiday)\n",
    "    # 使用 .corr() 方法，更安全\n",
    "    correlation = hol_sales['sales'].corr(holiday_indicator)\n",
    "    correlations[holiday] = correlation\n",
    "\n",
    "corr_df = pd.DataFrame(list(correlations.items()), columns=['holiday', 'sales_correlation']).fillna(0)\n",
    "# 3. 筛选出相关性强的假日\n",
    "selected_hols_df = corr_df[abs(corr_df['sales_correlation']) > 0.1]\n",
    "selected_holidays_list = selected_hols_df['holiday'].tolist()\n",
    "print(f\"    - 已筛选出 {len(selected_holidays_list)} 个重要假日。\")\n",
    "\n",
    "# 4. 对筛选出的假日进行独热编码\n",
    "hols_to_encode = holidays_filtered[holidays_filtered['description'].isin(selected_holidays_list)]\n",
    "holiday_dummies = pd.get_dummies(hols_to_encode[['date', 'description']], columns=['description'], prefix='hol')\n",
    "# 清理列名\n",
    "holiday_dummies.columns = [col.replace(\" \", \"_\").replace(\",\", \"\").replace(\"+\", \"_plus_\") for col in holiday_dummies.columns]\n",
    "# 按日期分组，处理同一天有多个假日的情况\n",
    "holiday_dummies = holiday_dummies.groupby('date').sum().reset_index()\n",
    "\n",
    "# 5. 将独热编码后的假日特征合并到主数据框\n",
    "df_featured = df_featured.merge(holiday_dummies, on='date', how='left')\n",
    "# 填充非假日产生的NaN为0\n",
    "dummy_cols = [col for col in holiday_dummies.columns if col != 'date']\n",
    "df_featured[dummy_cols] = df_featured[dummy_cols].fillna(0)\n",
    "\n",
    "\n",
    "# --- Part C: Lag, Rolling, and EWM Features ---\n",
    "print(\"    - 创建滞后、滚动和EWM特征...\")\n",
    "df_featured = df_featured.sort_values(by=['store_nbr', 'family', 'date'])\n",
    "lag_features = {'sales': [7, 14, 28, 35]} # 增加更多滞后\n",
    "for col, lags in lag_features.items():\n",
    "    for lag in lags:\n",
    "        df_featured[f'{col}_lag_{lag}'] = df_featured.groupby(['store_nbr', 'family'])[col].shift(lag)\n",
    "\n",
    "rolling_windows = [7, 14, 28, 60]\n",
    "for window in rolling_windows:\n",
    "    shifted_sales = df_featured.groupby(['store_nbr', 'family'])['sales'].shift(1)\n",
    "    df_featured[f'sales_rolling_mean_{window}'] = shifted_sales.rolling(window, min_periods=1).mean()\n",
    "    df_featured[f'sales_rolling_std_{window}'] = shifted_sales.rolling(window, min_periods=1).std()\n",
    "\n",
    "alphas = [0.95, 0.9, 0.8, 0.7]\n",
    "for alpha in alphas:\n",
    "    shifted_sales = df_featured.groupby(['store_nbr', 'family'])['sales'].shift(1)\n",
    "    df_featured[f'sales_ewm_alpha_{str(alpha).replace(\".\", \"\")}'] = shifted_sales.ewm(alpha=alpha).mean()\n",
    "\n",
    "\n",
    "# --- Part D: Final Imputation & Split ---\n",
    "print(\"    - 进行最终填充和数据分割...\")\n",
    "df_featured.fillna(0, inplace=True)\n",
    "df_train_final = df_featured[df_featured['id'].isin(train_df['id'])].copy()\n",
    "df_test_final = df_featured[df_featured['id'].isin(test_df['id'])].copy()\n",
    "print(\"✅ 特征工程完成！\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "#  Part 2: 单回归模型优化与训练\n",
    "# ===================================================================\n",
    "print(\"\\n⚙️ 步骤 2: 准备模型训练...\")\n",
    "features = [col for col in df_train_final.columns if col not in ['id', 'date', 'sales']]\n",
    "test_ids = df_test_final['id']\n",
    "X_train = df_train_final[features].copy()\n",
    "X_test = df_test_final[features].copy()\n",
    "y_train = np.log1p(df_train_final['sales'])\n",
    "\n",
    "# 更新分类特征列表 (不再需要旧的假日标志)\n",
    "categorical_features = [\"store_nbr\", \"family\", \"city\", \"state\", \"type\", \"dayofweek\", \"month\"]\n",
    "categorical_features = [f for f in categorical_features if f in features]\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# --- 创建验证集 ---\n",
    "val_start_date = '2017-07-26'\n",
    "val_mask = df_train_final['date'] >= val_start_date\n",
    "X_train_split, X_val_split = X_train[~val_mask], X_train[val_mask]\n",
    "y_train_split, y_val_split = y_train[~val_mask], y_train[val_mask]\n",
    "\n",
    "# --- Optuna 优化 ---\n",
    "print(\"\\n--- 开始优化回归模型 ---\")\n",
    "def objective_regressor(trial):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'boosting_type': 'gbdt',\n",
    "        'seed': 42, 'verbose': -1, 'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators=1500)\n",
    "    model.fit(X_train_split, y_train_split,\n",
    "              eval_set=[(X_val_split, y_val_split)],\n",
    "              eval_metric='rmse',\n",
    "              callbacks=[lgb.early_stopping(20, verbose=False)])\n",
    "    return model.best_score_['valid_0']['rmse']\n",
    "\n",
    "N_TRIALS = 15\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_regressor, n_trials=N_TRIALS)\n",
    "best_params = study.best_params\n",
    "print(f\"✅ 回归器优化完成！最佳 RMSE: {study.best_value:.5f}\")\n",
    "\n",
    "# --- 使用最佳参数在完整数据上训练最终模型 ---\n",
    "print(\"\\n--- 使用最佳参数在【完整数据】上训练最终模型 ---\")\n",
    "final_params = {**best_params, 'objective': 'regression_l1', 'metric': 'rmse', 'seed': 42, 'verbose': -1, 'n_jobs': -1}\n",
    "final_model = lgb.LGBMRegressor(**final_params, n_estimators=2500) # 增加最终训练的树数量\n",
    "final_model.fit(X_train, y_train,\n",
    "                eval_set=[(X_val_split, y_val_split)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[lgb.early_stopping(25, verbose=False)])\n",
    "\n",
    "# --- 生成最终预测 ---\n",
    "print(\"\\n⚙️ 开始生成最终预测...\")\n",
    "log_amount_pred = final_model.predict(X_test)\n",
    "final_predictions = np.expm1(log_amount_pred)\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'sales': final_predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ 提交文件 'submission.csv' 已成功生成！\")\n",
    "print(\"提交文件预览:\")\n",
    "print(submission_df.head())\n"
   ],
   "id": "96d5c65407ea2a38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ 步骤 1: 正在进行增强特征工程...\n",
      "    - 创建日期、趋势和傅里叶特征...\n",
      "    - 创建基于相关性的假日特征...\n",
      "    - 已筛选出 0 个重要假日。\n",
      "    - 创建滞后、滚动和EWM特征...\n",
      "    - 进行最终填充和数据分割...\n",
      "✅ 特征工程完成！\n",
      "\n",
      "⚙️ 步骤 2: 准备模型训练...\n",
      "\n",
      "--- 开始优化回归模型 ---\n",
      "✅ 回归器优化完成！最佳 RMSE: 0.37762\n",
      "\n",
      "--- 使用最佳参数在【完整数据】上训练最终模型 ---\n",
      "\n",
      "⚙️ 开始生成最终预测...\n",
      "\n",
      "✅ 提交文件 'submission.csv' 已成功生成！\n",
      "提交文件预览:\n",
      "           id     sales\n",
      "1684  3000888  0.405771\n",
      "1685  3002670  0.742122\n",
      "1686  3004452  0.543227\n",
      "1687  3006234  1.062653\n",
      "1688  3008016  0.679269\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312] *",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
